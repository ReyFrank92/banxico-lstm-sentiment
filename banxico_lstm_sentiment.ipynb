{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47de68d9",
   "metadata": {},
   "source": [
    "# Clasificación de tono monetario (Hawkish/Dovish/Neutral) con LSTM\n",
    "**Anexo del ensayo** — Repositorio listo para GitHub.  \n",
    "Última limpieza automática: 2025-10-02 13:26:23\n",
    "\n",
    "## Descripción\n",
    "Este cuaderno implementa una tubería (pipeline) de NLP para clasificar el tono de minutas del Banco de México en *hawkish*, *dovish* o *neutral* mediante una red LSTM.\n",
    "\n",
    "## Notas de uso\n",
    "- Este cuaderno se entrega **sin salidas** y con celdas de código limpias.\n",
    "- Todos los comentarios de código (`# ...`) fueron eliminados para dejar el código limpio.\n",
    "- Revise el archivo `README.md` para instrucciones reproducibles, dependencias y estructura del repositorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf5c77-12b6-4ea7-b7e2-686d90a74108",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ebe4a8-ee87-4e21-b167-ac5d8ba2b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def set_seed(seed: int = 13):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(2)   # ← cambia 13 por 7, 21, etc. para probar otras semillas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b952c4c-e80d-46ec-b155-9ded98d68fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253a9fb5-2285-4fe9-9b1d-81696825d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "carpeta = Path(r\"C:\\Users\\Scarl\\Documents\\CFA\\Curso\\Python Data Science and AI\\Unit 4\\Practice ensayo\")\n",
    "\n",
    "txt_files = sorted(carpeta.glob(\"*.txt\"))\n",
    "print(\"TXT encontrados:\")\n",
    "for i, p in enumerate(txt_files):\n",
    "    print(f\"[{i}] {p.name}\")\n",
    "\n",
    "idx_a_usar = 0  # <-- cambia este número si quieres otro archivo\n",
    "path = txt_files[idx_a_usar]\n",
    "print(\"\\nLeyendo:\", path)\n",
    "\n",
    "def leer_txt_robusto(p: Path):\n",
    "    errores = []\n",
    "    for enc in (\"utf-8-sig\", \"utf-8\", \"latin-1\"):\n",
    "        try:\n",
    "            return pd.read_csv(p, sep=None, engine=\"python\", encoding=enc)\n",
    "        except UnicodeDecodeError as e:\n",
    "            errores.append((enc, \"UnicodeDecodeError\"))\n",
    "        except pd.errors.ParserError as e:\n",
    "            for sep in [\"\\t\", \",\", \";\", \"|\"]:\n",
    "                try:\n",
    "                    return pd.read_csv(p, sep=sep, engine=\"python\", encoding=enc)\n",
    "                except Exception:\n",
    "                    continue\n",
    "            errores.append((enc, \"ParserError\"))\n",
    "        except Exception as e:\n",
    "            errores.append((enc, repr(e)))\n",
    "    raise RuntimeError(f\"No se pudo leer el archivo. Errores: {errores}\")\n",
    "\n",
    "df_txt = leer_txt_robusto(path)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 160)\n",
    "print(\"Shape:\", df_txt.shape)\n",
    "print(\"Columnas:\", list(df_txt.columns))\n",
    "df_txt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3631d0-ac03-48fc-9015-bbe275f54c0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df_txt[[\"sentences\", \"stance_label\"]].rename(\n",
    "    columns={\"sentences\": \"Text\", \"stance_label\": \"Label\"}\n",
    ")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06435b65-2103-4188-9cd4-d7c946221182",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ba2ed-5ff5-4d05-83b9-618aa731de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada02aa-e4ea-4b55-b424-f438da30b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1649b8a-751e-4d3b-a0eb-66390e9f2397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4ec809-7035-4092-873d-561f060f19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "_ = nltk.download('stopwords', quiet=True)  # silencia mensajes de descarga\n",
    "EN_STOPWORDS = set(stopwords.words('english'))  # no imprime al asignar\n",
    "print(f\"Stopwords cargadas: {len(EN_STOPWORDS)}\")  # confirmación breve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1687c3d9-9458-4d66-9bb9-64bbc9ebb311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('heshe')  # <- añadir tu stopword personalizada (en minúsculas)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    cleaned_text = ' '.join(words)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b5e50-b580-4101-a64a-5ba8354147d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned Text'] = df['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd116b2-62c8-4aaf-ba58-726fe5096aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = df['Label'].str.replace(r'^\\s*irrelevant\\s*$', 'neutral', case=False, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff29e3-f9f8-4548-a9fd-3a7821ea657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec9166-2b93-4d1c-b574-3878c17c4c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"df.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faeda81-2d9e-4111-8203-9da8df6ee1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def generate_word_cloud(text):\n",
    "    \n",
    "    custom_stopwords = {\"http\", \"china\", \"us\", \"united states\", \"political\", \"politics\",\"stock\",\"stocks\", \"trump\"}\n",
    "\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update(custom_stopwords)\n",
    "\n",
    "    wordcloud = WordCloud(width = 1600, height = 800, stopwords = stopwords, min_font_size = 10).generate(text)\n",
    "\n",
    "    plt.figure(figsize = (12, 12))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef9d0d-5037-447b-aaf3-c64bedf591e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud(\" \".join(df[df['Label'] == 'hawkish']['Cleaned Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04922635-5228-4f7b-816e-f312bffedef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud(\" \".join(df[df['Label'] == 'neutral']['Cleaned Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586b752b-0c12-4e83-bb89-6993130a7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud(\" \".join(df[df['Label'] == 'dovish']['Cleaned Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3891d-138d-44e6-990c-3a4e36ac4877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319af4c-3143-4352-b89f-6e54b3c5903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e828f78-640b-4caa-900e-a9b78f30d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37600bd1-f310-4fdd-9b86-da462fc339e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_padding(df):\n",
    "\n",
    "    df['Encoded Text'] = [tokenizer.encode(news) for news in df['Cleaned Text'].tolist()]\n",
    "\n",
    "    encoded_news_tensor = [torch.tensor(encoded_news) for encoded_news in df['Encoded Text'].tolist()]\n",
    "    padded_sequence = pad_sequence(encoded_news_tensor, batch_first = True, padding_value = 0).numpy()\n",
    "\n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c44aa-ea9c-4d0e-8f61-92a1b4f60c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenization_padding(df)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf5fae-34f1-4ed8-ac54-a1036b782f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437130a1-e88b-4eb2-a763-49e0ef89141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9395fe-b3bd-430d-865e-d34c141f9700",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Encoded Label'] = df['Label'].replace('hawkish', 0).replace('dovish', 1).replace('neutral', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7542c59-c2fd-42d5-94da-cb987d705239",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Encoded Label'] \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7dd77f-77fe-423c-96b5-13c472f1beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, shuffle = True)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size = 0.5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8798ec-5812-4c1b-96c1-f2bf3702c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6cab6c-0b8c-4eb3-bd0b-32255af9f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0bc889-99f4-48e2-b36c-539913eccb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inputs = tf.keras.Input(shape=(X_train.shape[1],), dtype=\"int32\")\n",
    "\n",
    "x = tf.keras.layers.Embedding(\n",
    "    input_dim=tokenizer.vocab_size,\n",
    "    output_dim=128,\n",
    "    mask_zero=True\n",
    ")(inputs)\n",
    "\n",
    "x = tf.keras.layers.SpatialDropout1D(0.25)(x)\n",
    "\n",
    "x = tf.keras.layers.LSTM(\n",
    "    32, return_sequences=True, activation='tanh', dropout=0.20\n",
    ")(x)\n",
    "\n",
    "\n",
    "gmax = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "gavg = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = tf.keras.layers.Concatenate()([gmax, gavg])   # ← nuevo pooling combinado\n",
    "\n",
    "\n",
    "outputs = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b530b-1415-4fad-bd9e-b404bb355f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2881ed4-8155-4020-8bd0-fd9313b81604",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'sparse_categorical_crossentropy', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80211bc9-67e5-45f3-a2c5-0cf3aad0f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741abc7-1677-41e7-97a7-010bbfaae765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    validation_data = (X_val, y_val),\n",
    "                    batch_size = 64, #32 original\n",
    "                    verbose = 1,\n",
    "                    epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c12192-6915-4ed0-b8b3-dac43d2c9412",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da3e83-4c1e-4b9b-88b0-33bc19eca1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db34b4-dd93-43ac-b944-e29f1e5bdece",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13292586-fa19-4320-99b6-cbc46584d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_predict = []\n",
    "\n",
    "for i in predictions:\n",
    "  y_predict.append(np.argmax(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd88ef9-33ce-4665-9baf-0a5f5d881874",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef25cca-e241-4e5b-a242-99679ce2e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6fd0e-3980-48dd-bac7-865f1beb644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "sns.heatmap(cm, fmt = 'd', annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdc6d14-d60b-4d6e-bb76-544e8930e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
